---
layout: post
title: Modeling Extreme Events in Time Series Prediction (KDD 2019)
date: 2020-02-13 00:00:00 +0000
description: Extreme Valuee Loss
img: /evl/profile.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [PaperReview, Distiributional Losses, Time Series]
---

<br/>

<br/>

최근의 연구들에서 범용적으로 사용된던 quadratic loss의 한계점을 지적하는 경우가 많다.  
  
해당 논문은 기존 로스의 한계점을 꽤 논리적으로 분석했고 그럴듯한 대안을 제시한다.  
    
저자가 수학적 조예가 깊은 사람이라 읽는데 조금 어려움은 있었지만 접근법과 성능면에서 분명 읽을 가치가 있는 논문이다.  
  
여기서 제안된 <b>Extreme Valuee Loss</b>는 risk management 분야에서 자주 등장하는 EVT(extreme value theorem)에 기초하고 있다.  
  
따라서 논문을 읽기 앞서 GRU(gate recurrent unit), EVT, Binary Cross Entropy의 개념을 보고 온다면 좋을 것 같다.  
  
<br/>

<br/>

## 00.  Paper Summary  
#### Contribution 01.  
Formal analysis on why deep neural network suffers extreme problems during predicting time series data with extreme value  
<br/>  
#### Contribution 02.  
Propose a novel loss function called Extreme Value Loss (EVL) based on extreme value theory  
<br/>  
#### Contribution 03.  
Propose a brand-new Memory Network based neural architecture to memorize extreme events in history  

<br/>

<br/>

## 01.  Preliminaries
#### Time Series Prediction  
Optimization Goal: 과거 1d 정보를 인풋으로 실제와 예측의 차이를 최소화하도록 모델을 학습  
  
Quadratic loss를 최소화하는 것을 기본으로 많은 모델이 등장, 최근에는 LSTM에서 forget term을 추가한 GRU가 많이 쓰이고 있다.  
<br/>  
#### Extreme Events  
일반적으로 실제 값을 기준으로하여 +/- 방향으로 임의의 threshold를 설정하고 {-1,0,1}로 발생 여부를 분류한다.  
  
#### Heavy-Tail Distribution  
극값을 가질 확률이 큰 분포, 범용적으로 사용되는 Gaussian / Poisson 분포는 light-tail Distribution  
  
일반적으로 light - tail 형태를 가정하고 모델을 학습하면 평균은 잘 맞출지 몰라도 꼬리 부분의 정보를 손실하게 된다.  
<br/>
#### EVT (Extreme Value Theory)  
EVT studies the distribution of maximum in observed sample  
  
Maximum of Random Variable의 분포를 추정하는 것을 목적으로 한다.  

<b>간단히 생각해서 정규 분포의 꼬리 부분을 뚝 자른 후, 잘라진 부분의 분포를 추정하는 것이라 볼 수 있다.</b>  
  
Maximum distribution의 y를 추정하는 방식은 (1) Block maxima, (2) Peaks over threshold 두 가지로 나눌 수 있는데  
  
각 방식에 따라 Generalized된 확률변수의 값이 조금 달라지게 된다. 해당 논문에서는 (1)의 방식을 택했고 아래와 같다.  
  
<center><img src="/assets/img/evl/evlone.jpg"></center>  
<br/>
여기서 분포의 형태를 결정하는 감마는 하이퍼 파라미터로 모델의 학습 과정에서 우리가 결정해주면 된다.  
  
확률 변수를 정의하고 선형 변환을 통해 분포를 추정하는 과정은 고민하지 않는 것이 정신 건강에 좋다. 눈으로 보면 아래와 같다.  
  
<center><img src="/assets/img/evl/evltwo.jpg"></center>  
<center>감마의 값이 커질수록 꼬리가 두꺼운 분포가 된다</center>  
<br/>  
  
#### Modeling the tail  
일반화한 분포를 사용하여 Extreme Event가 발생할 수 있는 확률에 대한 모델링이 가능하다.  
  
아래의 크사이도 역시 하이퍼 파라미터로 이상치라 판단할 수 있는 임계값이라 볼 수 있다.  
  
<center><img src="/assets/img/evl/evlthree.jpg"></center>  
<br/>  

#### Empirical Distribution After Optimization  
최적화의 목적함수인 minimizing quadratic loss는 x에 대한 y의 조건부 확률의 likelihood를 maximize하는 것과 동치이다.  
  
따라서 문제를 sample에 대한 조건부 확률값을 모두 곱한 것을 최대화하는 것으로 치환하고 나면  
  
이것이 non parametric Gaussian Kerenl을 통해 KDE를 수행하는 것과 같다고 한다. (그냥 납득하고 넘어간다 자세한 설명이 없다)  
  
이것은 Extreme Modeling을 수행하는데 많은 제약을 갖게 만든다고 저자는 말한다. 그리고 이제서야 본론으로 넘어갈 수 있다.  

<br/>

<br/>

## 02.  Why Deep NN could suffer Extreme Event Problem
Since nonparametric kernel density estimator only works well with sufficient samples  
the performance therefore is expected to decrease at the tail part of the data  
  
여기서의 Extreme Event Problem은 모델이 극단값에 대해 과적합하거나 아예 학습하지 못하는 문제를 말한다.  

- (1) Underfitting Problem: 극단값을 학습하지 못해 예측값이 평균 근처에서 맴도는 현상  
  
- (2) Overfitting Problem: 극단값을 너무 민감하게 받아들여서 시도때도 없이 극한값이라고 예측해버리는 현상  
  
두 가지 문제 모두 치명적이라고 볼 수 있는데 딥러닝 모델이 위의 문제들을 겪을 수 밖에 없는 이유를 논문에서 분석했다.  
  
앞서, quadratic loss를 통해 모델링을 하는 것은 Gaussian 분포를 가정하고 최적화되는 것이라 언급했다.  
  
그렇다면 예를 들어 두 샘플 x1, x2의 실제값이 1과 2이고 우리가 설정한 Extreme Value가 2 이상의 값이라고 가정해보자.  
  
모델을 최적화함에 있어 극단값의 것들이 반영되지 않았고 모델은 실제의 분포보다 light - tail의 정규분포를 출력할 것이다.  
  
<center><img src="/assets/img/evl/evlthree.jpg"></center>  
  
이것을 위와 같이 표현할 수 있고, 같은 방법으로 x2에 대한 y2의 확률값을 추정하면 true Value보다 작은 값을 출력할 것이다.  
  
이 말은 2보다 큰 y 값들에 대해서는 실제의 값보다 더 작은 값을 추정한다는 것이고, 곧 underfitting problem이라는 뜻이다.  
  
앞선 연구들은 극단값들을 모델에 반복 학습시켜 문제를 해결하고자 했는데 적절한 수치를 넘겨버리면 바로 overfitting 해버리고 말았다.  
  
적절한 가중을 가해 Extreme Event Problem을 해소하면서도 성능 좋은 예측 모델을 만들 수 있는 방법이 논문에 제안되었다.  
  
핵심적인 아이디어는 두 가지 Facotr로 볼 수 있다.  
<br/>
#### Memorizing Extrem Events  
Use memory network to memorize the characteristic of extreme events in history (새로운 뉴럴넷 스트럭쳐를 제안)  
  
#### Modeling Tail Distribution  
Approximated tailed distribution on observations and provide a novel classification called Extreme Value Loss (EVL)
  
<br/>

<br/>
## 03.  Memory Network Module




