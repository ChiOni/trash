---
layout: post
title: Why Should I Trust You? (KDD 2016)
date: 2020-01-29 00:00:00 +0000
description: Expaining the Predictions of Any Classifier
img: /lime/profile.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [PaperReview, ExplainableAI]
---

[“Why Should I Trust You?” Explaining the Predictions of Any Classifier(KDD 2016)](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)  
   
  
압도적인 성능을 무기로 블랙 박스 모델은 여러 분야에서 기존의 통계적 방법론들을 대체하고 있습니다.  
하지만 실험 데이터에게는 무자비하던 딥러닝 모델들이 현실 세계의 데이터를 마주했을 때 갑자기 멍청해지는 경우를 자주 볼 수 있는데요  
서비스로서 모델이 실력을 발휘해야 할 순간에 답답한 모습을 보인다면 곤란해지겠죠?  
  
우리가 개발한 모델은 어디서든 좋은 성능을 낼 것이라 신뢰한다.  
우리가 개발한 모델이 제시한 결과물을 신뢰한다.  
  
두 가지 믿음을 가질 수 있는가가 바로  <b>논문의 Motivation Trust Problem 입니다!</b> 
  
  
 
![](/assets/img/lime/limeone.jpg)  
위 실험은 어떤 경우에 우리가 모델을 신뢰할 수 없는지 설명합니다.  
  
<b>Task</b>  메일의 수신자가 기독교인지 무교인지 구별하는 과제가 주어졌습니다.
<b>Problem</b>  Train Set에 대해 높은 정확도를 보이던 모델에게 중요한 Feature는 발신자의 이름이었네요? 
<b>Result</b>  발신자 이름이 다양한 현실 세계의 데이터에서 위 모델은 실패할 것으로 보입니다.  
  
  
![](/assets/img/lime/limetwo.jpg)    
위의 과정이 저자가 궁극적으로 추구하는 의사결정의 과정입니다.  
- Flu가 정답이라는 근거는 sneeze / headache  
- Flu가 정답이 아닐 수 있는 이유는 no fatigue  
  
환자의 기록을 통해 증상을 예측하는 모델의 결과물을 의사는 어떻게 신뢰할 수 있을까요?  
만약 모델이 친절하게도 우리가 이해할 수 있는 수준의 근거를 제시한다면 사용자가 모델을 신뢰할 수 있지 않을까요?  
  
  
기존에 연구자들은 ML 모델을 신뢰하기 위해 3가지 정도의 방법을 사용했습니다.  
  
1. 결과물을 해석할 수 있는 얕은 tree model 등을 사용하거나  
2. accuracy 등의 스코어를 신뢰하거나  
3. A/B Testing을 통해 현실의 유저 피드백을 받는 것이었습니다.  
  
위의 3가지 방법들은 모두 성능이 좋지 않거나 비용이 많이 드는 등의 문제가 있었습니다.
그리고 이런 문제점들을 극복하고자 논문에서 제시한 것이 바로 <b>LIME (Locally Interpretable Model-Agnostic)입니다.</b>  
    
  
이름만으로 모델을 해석해보자면 LIME은  
- <b>Locally</b> / Global한 해석이 아니라 지협적인 설명을 제시하고
- <b>Interpretable</b> / 인간이 이해할 수 있는 수준의 방법으로
- <b>Model-Agnostic</b> / 어떤 Black-Box 모델에도 적용 가능한 방법론일 것 같습니다.





