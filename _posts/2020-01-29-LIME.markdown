---
layout: post
title: Why Should I Trust You? (KDD 2016)
date: 2020-01-29 00:00:00 +0000
description: Expaining the Predictions of Any Classifier
img: /lime/profile.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [PaperReview, ExplainableAI]
---

[“Why Should I Trust You?” Explaining the Predictions of Any Classifier(KDD 2016)](https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf)  
   
  
압도적인 성능을 무기로 블랙 박스 모델은 여러 분야에서 기존의 통계적 방법론들을 대체하고 있습니다.  
하지만 실험에서는 무자비하던 딥러닝 모델들이 현실의 데이터를 마주했을 때 갑자기 멍청해지는 경우를 자주 볼 수 있는데요  
서비스로서 모델이 실력을 발휘해야 할 순간에 답답한 모습을 보인다면 곤란해지겠죠?  
<br/>
우리가 개발한 모델은 어디서든 좋은 성능을 낼 것이라 신뢰한다.  
우리가 개발한 모델이 제시한 결과물을 신뢰한다.  
<br/>
#### 두 가지 믿음을 가질 수 있는가가 바로 논문의 Motivation Trust Problem 입니다!
<br/>
<br/>
<br/>
![](/assets/img/lime/limeone.jpg)   
<br/>
#### 위 실험은 어떤 경우에 우리가 모델을 신뢰할 수 없는지 설명합니다.  
<br/>
<br/>
<b> Task </b>  메일의 수신자가 기독교인지 무교인지 구별하는 과제가 주어졌습니다.    
<b> Problem </b>  Train Set에 대해 높은 정확도를 보이던 모델에게 중요한 Feature는 발신자의 이름이었네요?    
<b> Result </b>  발신자 이름이 다양한 현실 세계의 데이터에서 위 모델은 실패할 것으로 보입니다.    
<br/>
<br/>
<br/>
![](/assets/img/lime/limetwo.jpg)     
<br/>
<br/>
위의 과정이 저자가 궁극적으로 추구하는 의사결정의 과정입니다.  
<br/>
- Flu가 정답이라는 근거는 sneeze / headache  
- Flu가 정답이 아닐 수 있는 이유는 no fatigue  
<br/>
<br/>
환자의 기록을 통해 증상을 예측하는 모델의 결과물을 의사는 어떻게 신뢰할 수 있을까요?  
만약 모델이 친절하게도 우리가 이해할 수 있는 수준의 근거를 제시한다면 사용자가 모델을 신뢰할 수 있지 않을까요?  
<br/>
<br/>
<br/>
기존에 연구자들은 ML 모델을 신뢰하기 위해 3가지 정도의 방법을 사용했습니다.  
<br/>
1. 결과물을 해석할 수 있는 얕은 tree model 등을 사용하거나  
2. accuracy 등의 스코어를 신뢰하거나  
3. A/B Testing을 통해 현실의 유저 피드백을 받는 것이었습니다.  
<br/>
<br/>
위의 3가지 방법들은 모두 성능이 좋지 않거나 비용이 많이 드는 등의 문제가 있었습니다.  
그리고 이런 문제점들을 극복하고자 논문에서 제시한 것이 바로 <b>LIME (Locally Interpretable Model-Agnostic)입니다.</b>  
<br/>  
<br/>
이름만으로 모델을 해석해보자면 LIME은  
- <b>Locally</b> / Global한 해석이 아니라 지협적인 설명을 제시하고  
- <b>Interpretable</b> / 인간이 이해할 수 있는 수준의 방법으로  
- <b>Model-Agnostic</b> / 어떤 Black-Box 모델에도 적용 가능한 방법일 것 같습니다.  
<br/>
<br/>
직관적인 예시를 하나 보겠습니다. (사실은 이게 전부입니다 ㅎㅎ)  
![](/assets/img/lime/limethree.jpg)  
<br/>
<br/>
<b> Task </b> 한 점을 찍어서 그 점이 빨강인지 파랑인지 분류하는 문제가 있습니다.  
<b> Problem </b>  모델은 굵은 십자가를 빨강으로 예측했지만 우리가 구불구불한 곡선이 어떻게 생겨먹은지 이해할 수 없습니다.  
<b> Answer </b>  그런데 아주 간단한 점선을 하나 그어봤더니 얼추 저 곡선이랑 비슷한 결과물을 내는 것 같습니다.  
<br/>
#### 즉, LIME은 구불구불한 곡선을 설명할 수 있는 간단한 직선을 찾는 방법입니다.  
<br/>
<br/>
#### LIME을 구현하기 위해 필요한 과정은 딱 4가지 밖에 없습니다.  
<br/>
1. Input을 간단한 형태로 가공한다.  -- x를 x'로 가공한다.  
<br/>
2. 가공된 Input과 비슷한 데이터를 창조(sampling)한다. -- x'와 유사한 z' 를 만든다.  
<br/>
3. 창조된 데이터를 복잡한 형태로 가공한다. -- z'를 z로 가공한다.  
<br/>
4. 복잡한 모델과 유사한 결과물을 내는 간단한 모델을 만든다. -- f(z) = g(z')인 g를 찾는다.
<br/>
<br/>
그럼 이제 하나 하나의 과정을 다시 천천히 봐보도록 하겠습니다.  
<br/>
#### Input을 간단한 형태로 가공한다.  
<br/>
상식적으로 아주아주 데이터가 복잡한데 (예를 들면 64 x 64 x 3의 이미지) 이것에 대한 분류를 간단히 설명하기는 쉽지 않을 것 같습니다. 
따라서 이 복잡하고 커다란 데이터를 0/1로만 이루어진 단순한 덩어리로 바꿔주는 작업이 필요합니다.  
<br/>
- Data가 텍스트라면 word embedding한 값이 아니라 그냥 단어 덩어리를 인풋으로  
- Data가 이미지라면 하나 하나의 픽셀을 다 넣는 것이 아니라 super pixel 등의 덩어리를 인풋으로  
<br/>
<br/>
#### 가공된 Input과 비슷한 데이터를 창조(sampling)한다.  
<br/>
우리의 복잡한 이미지가 이제는 x' = (1,1,1,1,1)의 단순한 벡터로 바꿔졌다고 생각합시다. 그럼 우리는 samping을 통해 z'들을 뽑아냅니다.   
<br/>
- z' = [z'(1), z'(2), z'(3)] = [(1,1,1,1,0), (1,1,1,0,1), (1,1,1,0,0)]
- 여기서 딱 하나 기억할 것은 z'(1)보다는 z'(3)이 x'와 더 <b>멀리</b> 있는 Sample이라는 것입니다.  
<br/>
<br/>
#### 창조된 데이터를 복잡한 형태로 가공한다.  
<br/>
z'를 z로 바꿔주는 과정입니다. 설명의 편의를 위해 x가 이미지라고 생각해봅시다.  
z(1)은 x'에서 5번째 원소가 있었어야 할 부분이 회색으로 칠해진 이미지가 됩니다.  
그럼 복잡한 모델 f가 z(1)을 분류할때는 조금 다른 결과를 낼 수도 있겠죠?  
<br/>
<br/>
#### 복잡한 모델과 유사한 결과물을 내는 간단한 모델을 만든다.
<br/>
![](/assets/img/lime/limefour.jpg)   
<br/>
쉬운 모델의 학습을 이해하기 위한 유일한 수식입니다.   
모델은 학습을 거듭하며 f(z) = g(z')이 되도록 진화할 것입니다.  
그리고 기존 L2 loss와 조금 다른 점은 바로 저 앞에 붙은 파이입니다.  
멀리 있는 sample은 loss에 영향을 조금 끼치도록 거리가 inverse exponential하게 가중됩니다.
<br/>
<br/>
모델의 학습이 완료됬습니다!  
앞서 우리는 x'를 길이 5의 벡터로 가정했기 때문에 모델 g의 피쳐는 5개입니다.  
그럼 우리는 5개의 피쳐 중 어떤 것이 혹은 몇 개가 중요한지에 대해 Lasso를 통해 추출할 수 있습니다.  
<br/>
#### 만약 3번째가 가장 중요한 피쳐라면, 이미지의 분류에 가장 중요한 곳은 3번째 덩어리가 있는 부분일 것입니다.  
<br/>
<br/>
![](/assets/img/lime/limefive.jpg)  
<br/>
아직도 납득이 가지 않은 저를 위해 논문에서 아주 직관적인 예시를 제공해줬습니다.  
<br/>
이미지 분류 모델은 맨 왼쪽의 원본 사진을 보고 (전자 기타 > 통기타 > 강아지) 세 가지 정답을 제시했습니다.  
이 모델을 LIME으로 근사하고, 각 정답이 유도되는데 이미지의 어떤 덩어리가 중요했는지 역으로 찾아봤습니다.    
만약 전자기타를 정답으로 예측해놓고 중요한 부분은 강아지 얼굴이라고 제시했다면 우리는 모델을 신뢰할 수 없을겁니다.
모델이 아주 그럴듯한 부분을 중요하다고 콕 찝어줬다는 것이 보이시나요?  
<br/>
넵 신뢰할 수 있을 것 같습니다ㅎㅎ     
<br/>
<br/>
