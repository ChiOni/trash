---
layout: post
title: (PyTorch) Auto Encoder 
date: 2020-02-13 00:00:00 +0000
description: Extreme Valuee Loss
img: /pe/ae/profile.jpg 
fig-caption:   
categories: [PE]
tags: [Code Excercise, Auto Encoder, Anomaly Detection]

---
<br/>
  

## 00.  Dimensional Reduction  
<br/>
그저 데이터가 크고 복잡하면 도움이 될 것이라는 막연한 믿음은 여러 실험과 연구를 통해 부정당했다.  
  
단순히 저장과 처리가 힘들다는 문제 이외에, 데이터의 sparse가 커진다는 문제는 차원의 저주라는 그로스크한 이름으로 관찰의 대상이었는데  
  
고차원의 데이터를 저차원으로 압축시켜 이런 문제를 해결하는 과제가 Dimensional Reduction Task이다.  
<br/>  
<center><img src="/assets/img/pe/ae/aeone.jpg"></center>
<center><small>차원이 커질수록 데이터가 차지하는 부분이 작아진다</small></center>
<br/>
Dimensional Reduction은 고전적으로 feature selection 이라는 과제와 동치로 여겨졌는데 Lasson Regression과 같은 것이 그 중 하나이다.  
  
최근에는 일부를 고르는 것이 아니라 전부를 대표할 수 있는 저차원의 정보를 뽑는다는 컨셉의 Latent Feature Extraction이 많이 사용된다.  
  
그리고 오늘은 그 중 딥러닝 모델에서 Latent Feature Extraction에 가장 범용적으로 사용되는 AutoEncoder를 구현해 보고자 한다.  
<br/>
  
<br/>
 
## 01. PCA (Principal Component Analysis)  
<br/>
2차원 평면의 점들을 하나의 축에 사영한 후에, 그 축 위에서의 관계만을 보겠다는 것이 차원 축소의 개념이라 볼 수 있다.  
  
오토인코더를 보기 전에 대표적인 Linear Latent Feature Extraction 기법인 PCA를 알고 가는 것이 좋을 것 같다.  
  
한글로는 주성분분석이라 불리우는 PCA는 데이터를 잘 표현하는 '축'을 찾는 기법이다.  
  
어렵게 말해보자면 mapping 이후 가장 큰 분산을 갖게 하는 unit vector 꼴의 orthogonal linear transformation을 찾는 기법이다.
<br/>  
<center><img src="/assets/img/pe/ae/aetwo.jpg"></center>
<br/>
PCA의 주어진 과제는 사영 후 분산을 최대화하는 w를 찾는 것인데  
  
이것은 인풋 X의 Covariance Matrix의 가장 큰 eigenvalue의 eigenvetor를 찾는 일과 동치이다.  
<br/>
<center><img src="/assets/img/pe/ae/aethree.jpg"></center>
<br/>
  
근데 이렇게 분산을 최대화하는 eigenvector를 찾는 과정은 쉽지 않은데  
  
다행히도 수학적으로 분산을 최대화하는 Task는 error를 minimize하는 Task와 동치이다.  
  
그리고 에러를 최소화하는 vector를 찾는 일을 딥러닝의 방식으로 해석한 것이 오토인코더라고 볼 수 있다.  
<br/>
<center><img src="/assets/img/pe/ae/aefour.jpg"></center>
<center><small>굳이 증명 과정을 한 줄 한 줄 이해할 필요가 있을까</small></center>
<br/> 
  
<br/>
  
## 02.  Auto Encoder  
<br/>
오토인코더는 Data를 가장 잘 재구성하도록 성장하는 모델이다.  
  
커다란 숫자 덩어리가 여러 layer를 지나며 수많은 행렬곱에 의해 줄었다 늘어났음에도  
  
원래의 숫자 덩어리 형태를 그대로 잘 유지할 수 있도록 모델은 학습된다.
<br/>
<center><img src="/assets/img/pe/ae/aefour.jpg"></center>
<br/>
  
#### 그런데 오토인코더는 왜 쓰는 것일까?  
  
제목에도 써놓은 당연한 소리지만 오토인코더는 Dimensional Reduction의 용도로 가장 많이 쓰인다.  
  
위의 사진과 같이 모델은 <b>Encoder / Decoder</b> 두 개의 네트워크로 구성된다.  
  
그리고 그 사이에는 Input의 차원보다 적은 크기의 Layer가 존재하는데 이것을 Latent Feature Space라 볼 수 있다.  
  
Decoder 부분을 생각해보면 모델은 낮은 차원의 벡터로 높은 차원의 벡터를 재구성(reconstruction)한다.  
  
그리고 Output이 Input과 동일하니 Latent Feature Space는 Input의 정보가 함축되어있는 공간이라 볼 수 있다.  
  
즉, 모델의 Encoder 부분은 고차원의 데이터를 저차원으로 사영하는 Dimensional Reduction 네트워크인 것이다.  
  
<br/>

PCA는 Encoder 부분이 w이고 Decoder 부분이 그 역행렬인 가장 간단한 형태의 오토인코더 모델이라고 볼 수 있다.  
  
그러나 네트워크의 Layer를 키우고 Activation이 행해지면 오토인코더는 더이상 PCA와 같이 선형적인 관계만을 보지 않는다.  
  
<b>정리하자면 오토인코더는 고차원 데이터의 Non Linear Dimensionality Reduction을 위해 사용된다.</b>  
  
<br/>  
#### 그러면 오토인코더는 어디에 쓸 수 있을까?  
  
가장 쉽게 오토인코더의 예제를 찾을 수 있는 것은 Anomaly Detection 분야이다.  
  
Anomal Data에 대한 가정은, Normal 데이터에 공통적으로 존재하는 feature가 그들에게 존재하지 않는다는 것이다.  
  
예를 들어, 금융 거래 데이터를 가지고 금융 사기범과 일반인을 구별하는 과제가 있다고 생각해보자.  
  
과거에는 이런 과제를 아주 휴리스틱한 룰 베이스의 시스템을 통해 수행해왔다.  
  
골프장에서 거래내역이 있으면 일반인, 월급 통장이 있으면 정상인, 이런 규칙들을 정해놓고 O / X 로 분류를 수행했다.  
  
이런 룰 하나 하나가 anomal 데이터에는 존재하지 않는 normal 데이터의 feature라 볼 수 있다.  
  
이것을 오토인코더의 관점에서 생각해보자, 모델은 input을 잘 reconstuction 하는 방향으로 학습된다.  
  
따라서 정상인의 데이터만으로 모델을 학습시킨다면 Encoder 네트워크는  
  
Latent Vector Space에 정상 데이터의 공통적인 Feature들이 잘 함축되도록 업데이트 될 것이다.  
  
그렇다면 학습이 완료된 모델에 anomal 데이터가 들어왔을 때, anomal 데이터에는 그런 Feature들이 없으니 재구성에 실패할 것이다.  

<br/>

정리하자면 오토인코더를 통한 Anomaly Detection의 학습 전략은 이렇다.  
  
<p style="text-indent :5em;" >(1) Encoder - Decoder로 이루어진 네트워크를 설계한다</p>  
  
<p style="text-indent :5em;" >(2) 모델에 normal 데이터를 잔뜩 집어넣고 Reconstrucion Loss를 최소화하도록 업데이트한다</p>  
  
<p style="text-indent :5em;" >(3) 적절한 Threshold를 설정하고 새로운 데이터의 에러가 임계값을 초과하면 Anomal Data로 분류한다</p>  
   
<br/>  
  
<br/>  

## 04. MNIST Aanomaly Detection



  

  



