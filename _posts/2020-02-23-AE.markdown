---
layout: post
title: (PyTorch) Auto Encoder 
date: 2020-02-13 00:00:00 +0000
description: Extreme Valuee Loss
img: /pe/ae/profile.jpg 
fig-caption:   
categories: [PE]
tags: [Code Excercise, Auto Encoder, Anomaly Detection]

---
<br/>
  

## 00.  Dimensional Reduction  
<br/>
그저 데이터가 크고 복잡하면 도움이 될 것이라는 막연한 믿음은 여러 실험과 연구를 통해 부정당했다.  
  
단순히 저장과 처리가 힘들다는 문제 이외에, 데이터의 sparse가 커진다는 문제는 차원의 저주라는 그로스크한 이름으로 관찰의 대상이었는데  
  
고차원의 데이터를 저차원으로 압축시켜 이런 문제를 해결하는 과제가 Dimensional Reduction Task이다.  
<br/>  
<center><img src="/assets/img/pe/ae/aeone.jpg"></center>
<center><small>차원이 커질수록 데이터가 차지하는 부분이 작아진다</small></center>
<br/>
Dimensional Reduction은 고전적으로 feature selection 이라는 과제와 동치로 여겨졌는데 Lasson Regression과 같은 것이 그 중 하나이다.  
  
최근에는 일부를 고르는 것이 아니라 전부를 대표할 수 있는 저차원의 정보를 뽑는다는 컨셉의 Latent Feature Extraction이 많이 사용된다.  
  
그리고 오늘은 그 중 딥러닝 모델에서 Latent Feature Extraction에 가장 범용적으로 사용되는 AutoEncoder를 구현해 보고자 한다.  
<br/>
  
<br/>
 
## 01. PCA (Principal Component Analysis)  
<br/>
2차원 평면의 점들을 하나의 축에 사영한 후에, 그 축 위에서의 관계만을 보겠다는 것이 차원 축소의 개념이라 볼 수 있다.  
  
오토인코더를 보기 전에 대표적인 Linear Latent Feature Extraction 기법인 PCA를 알고 가는 것이 좋을 것 같다.  
  
한글로는 주성분분석이라 불리우는 PCA는 데이터를 잘 표현하는 '축'을 찾는 기법이다.  
  
어렵게 말해보자면 mapping 이후 가장 큰 분산을 갖게 하는 unit vector 꼴의 orthogonal linear transformation을 찾는 기법이다.
<br/>  
<center><img src="/assets/img/pe/ae/aetwo.jpg"></center>
<br/>
PCA의 주어진 과제는 사영 후 분산을 최대화하는 w를 찾는 것인데  
  
이것은 인풋 X의 Covariance Matrix의 가장 큰 eigenvalue의 eigenvetor를 찾는 일과 동치이다.  
<br/>
<center><img src="/assets/img/pe/ae/aethree.jpg"></center>
<br/>
  
근데 이렇게 분산을 최대화하는 eigenvector를 찾는 과정은 쉽지 않은데  
  
다행히도 수학적으로 분산을 최대화하는 Task는 error를 minimize하는 Task와 동치이다.  
  
그리고 에러를 최소화하는 vector를 찾는 일을 딥러닝의 방식으로 해석한 것이 오토인코더라고 볼 수 있다.  
<br/>
<center><img src="/assets/img/pe/ae/aefour.jpg"></center>
<center><small>굳이 증명 과정을 한 줄 한 줄 이해할 필요가 있을까</small></center>
<br/> 
  
<br/>
  
## 02.  Auto Encoder  
<br/>
오토인코더는 Data를 가장 잘 재구성하도록 성장하는 모델이다.  
  
커다란 숫자 덩어리가 여러 layer를 지나며 수많은 행렬곱에 의해 줄었다 늘어났음에도  
  
원래의 숫자 덩어리 형태를 그대로 잘 유지할 수 있도록 모델은 학습된다.
<br/>
<center><img src="/assets/img/pe/ae/aefour.jpg"></center>
<br/>
  
#### 그런데 오토인코더는 왜 쓰는 것일까?  
  
제목에도 써놓은 당연한 소리지만 오토인코더는 Dimensional Reduction의 용도로 가장 많이 쓰인다.  
  
위의 사진과 같이 모델은 <b>Encoder / Decoder</b> 두 개의 네트워크로 구성된다.  
  
그리고 그 사이에는 Input의 차원보다 적은 크기의 Layer가 존재하는데 이것을 Latent Feature Space라 볼 수 있다.  
  
Decoder 부분을 생각해보면 모델은 낮은 차원의 벡터로 높은 차원의 벡터를 재구성(reconstruction)한다.  
  
그리고 Output이 Input과 동일하니 Latent Feature Space는 Input의 정보가 함축되어있는 공간이라 볼 수 있다.  
  
즉, 모델의 Encoder 부분은 고차원의 데이터를 저차원으로 사영하는 Dimensional Reduction 네트워크인 것이다.  
  
<br/>

PCA는 Encoder 부분이 w이고 Decoder 부분이 그 역행렬인 가장 간단한 형태의 오토인코더 모델이라고 볼 수 있다.  
  
그러나 네트워크의 Layer를 키우고 Activation이 행해지면 오토인코더는 더이상 PCA와 같이 선형적인 관계만을 보지 않는다.  
  
<b>정리하자면 오토인코더는 고차원 데이터의 Non Linear Dimensionality Reduction을 위해 사용된다.</b>  
  
<br/>
#### 그러면 오토인코더는 어디에 쓸 수 있을까?  
  
가장 쉽게 오토인코더의 예제를 찾을 수 있는 것은 Anomaly Detection 분야이다.  
  

  




  

  



