---
layout: post
title: Graph Neural Network
date: 2020-02-29 00:00:00 +0000
description: Node Classification
img: /pe/gnn/profile.jpg 
fig-caption:   
categories: [PE]
tags: [Code Excercise, Graph Neural Network, Node Classification]

---
<br/>
  
## 00. Graph is everything
<br/>
소셜 네트워크도 그래프다.  분자 구조도 그래프다. 넷플릭스 시청 내역도 그래프다. 

주체와 관계가 있는 모든 종류의 데이터는 그래프의 꼴로 치환이 가능하다.

하여튼 아주아주 그래프가 중요하다고들 하는데 난 그래프를 잘 모른다. 

전공 선택 과목이었던 '조합 및 그래프 이론' 을 듣지 않은 것이 이제와서 조금은 아쉽다.

<br/>
<center><img src="/assets/img/pe/gnn/gnnone.jpg"></center>
<br/>
  
고인물이 엄청 많은 그래프를 심도 있게 고찰하는 것은 포기하고 기본만 한 번 훝어보고자 한다.  
  
그래프는 주체가 되는 Node와 관계를 표현한 Edge로 이루어졌다. 관계는 Directed와 Undirected 관계로 나눌 수 있다.  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnntwo.jpg"></center>
<br/>
  
특정 노드에 붙어있는 edge의 숫자를 노드의 degree라고 표현하는데  
  
만약 전체 데이터에서 N개의 노드가 있다면 모든 노드의 연결관계와 degree를 N x N 매트릭스로 표현할 수 있다.  
  
표현된 매트릭스의 row sum은 해당 노드의 degree가 될 것이고, Undirected 그래프는 symmetric matrix가 될 것이다.  
  
알고리즘과 코딩 테스트를 따로 준비해본적은 없지만 DFS, BFS에 대해서는 많이도 들어봤다.  
  
그래프를 탐색하는 방식의 차이인데 깊게 혹은 넓게 볼 것인지의 차이라고만 이해하면 될 것 같다.  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnnthree.jpg"></center>
<br/>

<br/>

하여튼 그래프에서 ML을 통해 해결하고자 Network Task는 아래와 같은데 이것들을 해결하는 일이 쉽지가 않다.  
  
(1) Node Classification (Anomaly Detection, Fraud Detection ..)  
  
(2) Link Prediction (Recommender System)  
  
(3) Network Similarity  
  
<br/>
  
우선, 그래프 데이터는 이미지나 시계열과 같이 정형적인 형태의 데이터로 취급할 수 없다.  
  
네트워크라는 것이 이미지와 같이 고정된 크기의 것이 아니라 Input이 가변적이고  
  
시계열과 같이 선후관계가 있는 것이 아니라 노드들을 Ordering 하는 것이 불가능하기 때문이다.  
  
본격적으로 GNN 모델링을 하기에 앞서 그래프 데이터를 어떻게 임베딩할 것인지에 대한 기본 개념을 하나 알고 가자.  
  
<br/>

<br/>

## 01.  Random Walk Embeddings  
<br/>
데이터의 구조와 관계를 알지 못하는 상태에서 표본적으로 형태를 구체화하기 위해 Random Walk가 수행된다.
  
Random Walk는 아무 Node를 시작점으로 잡고 마구 움직여서 주위에 어떤 Node로 갈 수 있는지 탐색하는 기법이다.  
  
여기서 "마구 움직임"의 전략을 DFS 혹은 BFS로 선택함에 따라 "어디까지 갈 수 있는지"와 "어디로 많이 갈 것인지"를 탐색할 수 있다.  
  
이런 랜덤워크 프로세스를 활용하여 그래프 데이터를 임베딩하려는 시도가 <b>DeepWalk (KDD 2014)</b>에 있었는데  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnnfour.jpg"></center>
<br/>
  
컨셉적으로 이해해보자면, 각 Node를 d 길이의 벡터로 임베딩할 것인데,  
  
Random walk를 통해 가깝다고 나타단 노드와는 inner product값이 크고, 멀리 있는 노드와는 값이 작도록 학습될 것이다.  
  
그리고 이런 방향의 학습을 위해 skip gramm loss와 비용의 감소를 위해 negative sampling이 사용되는데 일단 넘어가자.  
  
그런데 DeepWalk에서는 Random walk를 정말로 random하게 취하여 그래프의 전체 Structure를 잘 캡쳐하지 못했었는데,  
  
이후에 나온 <b>node2vec (KDD 2016)</b> 논문에서 이것을 보완하여 dfs, bfs 전략을 융합했고, 현재에도 많이 사용되고 있다고 한다.  

<br/>

<br/>

## 02.  Graph Neural Network (GNN)


