---
layout: post
title: Graph Neural Network
date: 2020-02-29 00:00:00 +0000
description: Node Classification
img: /pe/gnn/profile.jpg 
fig-caption:   
categories: [PE]
tags: [Code Excercise, Graph Neural Network, Node Classification]

---
<br/>
  
## 00. Graph is everything
<br/>
소셜 네트워크도 그래프다.  분자 구조도 그래프다. 넷플릭스 시청 내역도 그래프다. 

주체와 관계가 있는 모든 종류의 데이터는 그래프의 꼴로 치환이 가능하다.

하여튼 아주아주 그래프가 중요하다고들 하는데 난 그래프를 잘 모른다. 

전공 선택 과목이었던 '조합 및 그래프 이론' 을 듣지 않은 것이 이제와서 조금은 아쉽다.

<br/>
<center><img src="/assets/img/pe/gnn/gnnone.jpg"></center>
<br/>
  
그래프를 뉴럴넷을 보기에 앞서 그래프에 대한 기본만 한 번 훝어보고자 한다.  
  
그래프는 주체가 되는 Node와 관계를 표현한 Edge로 이루어졌다. 관계는 Directed와 Undirected 관계로 나눌 수 있다.  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnntwo.jpg"></center>
<br/>
  
특정 노드에 붙어있는 edge의 숫자를 노드의 degree라고 표현하는데  
  
만약 전체 데이터에서 N개의 노드가 있다면 모든 노드의 연결관계와 degree를 N x N 매트릭스로 표현할 수 있다.  
  
표현된 매트릭스의 row sum은 해당 노드의 degree가 될 것이고, Undirected 그래프는 symmetric matrix가 될 것이다.  
  
알고리즘과 코딩 테스트를 따로 준비해본적은 없지만 DFS, BFS에 대해서는 많이도 들어봤다.  
  
그래프를 탐색하는 방식의 차이인데 깊게 혹은 넓게 볼 것인지의 차이라고만 이해하면 될 것 같다.  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnnthree.jpg" width = "400" height = "250"></center>
<br/>

<br/>

하여튼 그래프에서 ML을 통해 해결하고자 Network Task는 아래와 같은데 이것들을 해결하는 일이 쉽지가 않다.  
  
(1) Node Classification (Anomaly Detection, Fraud Detection ..)  
  
(2) Link Prediction (Recommender System)  
  
(3) Network Similarity  
  
<br/>
  
우선, 그래프 데이터는 이미지나 시계열과 같이 정형적인 형태의 데이터로 취급할 수 없다.  
  
네트워크라는 것이 이미지와 같이 고정된 크기의 것이 아니라 Input이 가변적이고  
  
시계열과 같이 선후관계가 있는 것이 아니라 노드들을 Ordering 하는 것이 불가능하기 때문이다.  
  
본격적으로 GNN 모델링을 하기에 앞서 그래프 데이터를 어떻게 임베딩할 것인지에 대한 기본 개념을 하나 알고 가자.  
  
<br/>

<br/>

## 01.  Random Walk Embeddings  
<br/>
데이터의 구조와 관계를 알지 못하는 상태에서 표본적으로 형태를 구체화하기 위해 Random Walk가 수행된다.
  
Random Walk는 아무 Node를 시작점으로 잡고 마구 움직여서 주위에 어떤 Node로 갈 수 있는지 탐색하는 기법이다.  
  
여기서 "마구 움직임"의 전략을 DFS 혹은 BFS로 선택함에 따라 "어디까지 갈 수 있는지"와 "어디로 많이 갈 것인지"를 탐색할 수 있다.  
  
이런 랜덤워크 프로세스를 활용하여 그래프 데이터를 임베딩하려는 시도가 <b>DeepWalk (KDD 2014)</b>에 있었는데  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnnfour.jpg"></center>
<br/>
  
컨셉적으로 이해해보자면, 각 Node를 d 길이의 벡터로 임베딩할 것인데,  
  
Random walk를 통해 가깝다고 나타단 노드와는 inner product값이 크고, 멀리 있는 노드와는 값이 작도록 학습될 것이다.  
    
그런데 DeepWalk에서는 Random walk를 정말로 random하게 취하여 그래프의 전체 Structure를 잘 캡쳐하지 못했었는데,  
  
이후에 나온 <b>node2vec (KDD 2016)</b> 논문에서 이것을 보완하여 dfs, bfs 전략을 융합했고, 현재에도 많이 사용되고 있다고 한다.  

<br/>

<br/>

## 02.  Graph Neural Network (GNN)
<br/>
N개의 노드의 skip gram loss를 최소화하는 N x d(# of feature) 매트릭스를 학습하는 임베딩 기법은 shallow encoding이라 불린다.  
  
이런 고전의 방법들은 몇가지 한계점이 있는데  
  
(1) 각 노드가 Parameter를 공유하지 않고  
  
(2) 학습에서 보지 않은 노드를 임베딩할 수 없고  
  
(3) 엣지 관계 이외의 노드의 특징을 임베딩 과정에 반영할 수 없었다.
<br/>
따라서 이런 한계점들을 극복하고자 고안된 "깊은" 임베딩 기법이 바로 Graph Neural Networks이다.  
  
GNN의 가장 큰 특징은 특정 Node의 Neighbor Node 정보를 Aggregate한 값을 임베딩에 활용하는 것이라 볼 수 있는데  
  
Aggregation에 대한 구체적인 이해를 위해 아래의 알고리즘을 천천히 봐 볼 필요가 있다.  
  
<br/>
<center><img src="/assets/img/pe/gnn/gnnfour.jpg"></center>
<br/>

우리가 타겟하는 노드가 v일때, v에게는 N(v)라는 이웃 노드 집합이 존재한다.  
  
4번 째 줄은 N(v)안에 있는 각각의 u 노드에 존재하는 N(u)의 정보를 취합하는 과정이며  
  
5번 째 줄은 u노드의 정보와 N(u)의 정보를 취합하는 과정이라는 것을 알 수 있다.  
  
즉, 0-depth node의 정보를 얻기 위해 1-depth의 정보가 들어가는데, 1-depth의 정보에는 2-depth와 3-depth의 정보가 들어가는 것이다.  
  
정리하자면 GNN은 각 노드에 특화한 임베딩 벡터를 업데이트 하는 것이 아니라 목표 노드의 이웃을 잘 취합하도록 학습된다.  
  
따라서 임베딩 과정에서 각 노드들이 Parameter를 공유할 수 있고 새로운 노드에도 바로 적용할 수 있다는 장점이 있다.  
 
<br/>

<br/>

## 03.  Node Classfication Task (Example)  
<br/>


  



